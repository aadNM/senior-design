import json
import logging
import operator
import os
import time
from collections import OrderedDict, defaultdict
from typing import Any, Dict, Iterable, List, Optional, Set, Text, Tuple, Union

import rasa.shared.nlu.training_data.loading
import rasa.shared.nlu.training_data.training_data as nlu_data
import rasa.shared.nlu.training_data.util
import rasax.community.config as rasa_x_config
import rasax.community.constants as constants
import rasax.community.services.websocket_service as websocket_service
import rasax.community.utils.cli as cli_utils
import rasax.community.utils.common as common_utils
import rasax.community.utils.io as io_utils
from rasa.shared import data as rasa_data
from rasa.shared.nlu.constants import INTENT, INTENT_RESPONSE_KEY
from rasa.shared.nlu.training_data import loading
from rasa.shared.nlu.training_data.formats import MarkdownReader, RasaYAMLReader
from rasa.shared.nlu.training_data.formats.rasa import RasaReader
from rasa.shared.nlu.training_data.formats.rasa_yaml import KEY_NLU
from rasa.shared.nlu.training_data.formats.readerwriter import TrainingDataWriter
from rasa.shared.nlu.training_data.training_data import TrainingData as NluTrainingData
from rasax.community.database.conversation import MessageLog
from rasax.community.database.data import (
    EntitySynonym,
    EntitySynonymValue,
    LookupTable,
    RegexFeature,
    Response,
    is_trainable_extractor,
    TrainingData,
    TrainingDataEntity,
)
from rasax.community.database.service import DbService
from rasax.community.services import background_dump_service
from rasax.initialise import _read_data
from sanic.request import Request
from sqlalchemy import and_

logger = logging.getLogger(__name__)


class DataError(Exception):
    """Base exception for all Data errors."""


class TrainingDataNotFound(DataError):
    """Exception that is raised when training data cannot be found."""


class TrainingDataUpdateFailed(DataError):
    """Exception that is raised when training data cannot be replaced."""


class TrainingDataCreationFailed(DataError):
    """Exception that is raised when training data cannot be saved."""


def unique_dictionaries(dicts: List[Dict[Text, Any]]) -> List[Dict[Text, Any]]:
    """Given a list of dictionaries, return a list of unique dictionaries."""
    out = []
    unique_json = set()

    for d in dicts:
        dict_json = json.dumps(d)
        if dict_json not in unique_json:
            out.append(d)
            unique_json.add(dict_json)

    return out


class DataService(DbService):
    def get_training_data(
        self,
        project_id: Text,
        sort_by_descending_id: bool = False,
        sort_by: Optional[Text] = None,
        sort_order: Optional[Text] = None,
        text_query: Optional[Text] = None,
        intent_query: Optional[Text] = None,
        entity_query: bool = False,
        fields_query: List[Tuple[Text, bool]] = None,
        filename: Optional[Text] = None,
        limit: Optional[int] = None,
        offset: Optional[int] = None,
        distinct: bool = True,
    ) -> common_utils.QueryResult:
        """Retrieves the NLU training data for a certain project.

        Args:
            project_id: The project which the data belongs to.
            sort_by_descending_id: If `True` the training data is sorted by descending
                id, if `False` it is sorted by ascending id.
            sort_by: Field to which sort results by.
            sort_order: Order in which results should be returned in, ascending or
                descending. Only applies when `sort_by` has been specified.
            text_query: Only return training examples which contain this text.
            intent_query: Only return training examples which contain one of the
                specified intents (specify multiple intents by separating them
                with `,`).
            entity_query: Return a list of all entities in the training data.
            fields_query: Only return certain fields of the training examples.
            filename: Only return training examples from a specific file.
            limit: Return a maximum number of `limit` training examples.
            offset:  Return training examples after the `offset`th row in the database.
            distinct: If `True` exclude duplicate entries.

        Returns:
            The paginated training examples matching the given query and the total
            number of results for the given query.
        """
        if entity_query:
            entities = sorted(
                self.get_entities(project_id), key=operator.itemgetter("entity")
            )
            return common_utils.QueryResult(entities, len(entities))

        if intent_query:
            query = TrainingData.intent.in_(intent_query.split(","))
        else:
            query = True

        if text_query:
            query = and_(query, TrainingData.text.ilike(f"%{text_query}%"))

        if filename:
            query = and_(query, TrainingData.filename == filename)

        examples = (
            self.query(TrainingData)
            .filter(TrainingData.project_id == project_id)
            .filter(query)
        )

        total_number_examples = examples.count()

        if sort_by_descending_id:
            examples = examples.order_by(TrainingData.id.desc())
        else:
            examples = examples.order_by(TrainingData.id.asc())

        sort_order = sort_order or "desc"
        if sort_order not in ["desc", "asc"]:
            raise ValueError(
                f"Value for `sort_order` must be `asc` or `desc` (got: '{sort_order}')."
            )

        if sort_by == "time":
            sort_by = "annotated_at"
        sort_column = TrainingData.__table__.columns.get(sort_by or "id")
        if sort_column is None:
            raise ValueError(f"Invalid column '{sort_by}' for MessageLog.")

        examples = examples.order_by(
            sort_column.desc() if sort_order == "desc" else sort_column.asc()
        )

        examples = examples.offset(offset).limit(limit)
        if distinct:
            examples = examples.distinct()
        examples = [t.as_dict() for t in examples]

        if fields_query:
            # Field queries can specify relationship items hence we rather get
            # the full objects and then filter out the fields we want.
            examples = [
                common_utils.filter_fields_from_dict(e, fields_query) for e in examples
            ]
            if distinct:
                examples = unique_dictionaries(examples)

        return common_utils.QueryResult(examples, total_number_examples)

    def create_formatted_training_data(
        self,
        project_id: Text,
        file_name: Optional[Text] = None,
        should_include_lookup_table_entries: bool = False,
    ) -> Dict[Text, Dict[Text, Any]]:
        """Return training data in NLU format.

        Args:
            file_name: Only get data of a certain file.
            project_id: Project id of the new training example.
            should_include_lookup_table_entries: If `True` the training data includes
                the entries of `LookupTable`s. If `False` it just includes the
                abbreviated form with a file reference as element.

        Returns:
             Combined contents of `training_data`, `regex_features`,
            `entity_synonyms` and `lookup_tables`.
        """

        training_examples, _ = self.get_training_data(project_id, filename=file_name)
        regex_features, _ = self.get_regex_features(project_id, filename=file_name)
        if should_include_lookup_table_entries:
            lookup_tables = self.get_lookup_tables_with_elements(
                project_id, filename=file_name
            )
        else:
            lookup_tables = self.get_lookup_tables(project_id, filename=file_name)
        entity_synonyms = self.get_entity_synonyms(project_id, filename=file_name)
        return nlu_format(
            training_examples, regex_features, lookup_tables, entity_synonyms
        )

    def get_all_filenames(self, project_id: Text) -> List[Text]:
        """Return a list of all values of `filename`."""
        training_data_filenames = (
            self.query(TrainingData.filename)
            .filter(TrainingData.project_id == project_id)
            .distinct()
            .all()
        )

        training_data_filenames = {row[0] for row in training_data_filenames}

        regex_filenames = (
            self.query(RegexFeature.filename)
            .filter(RegexFeature.project_id == project_id)
            .distinct()
            .all()
        )
        regex_filenames = {row[0] for row in regex_filenames}

        lookup_filenames = (
            self.query(LookupTable.referencing_nlu_file)
            .filter(LookupTable.project_id == project_id)
            .distinct()
            .all()
        )
        lookup_filenames = {row[0] for row in lookup_filenames}

        synonym_filenames = (
            self.query(EntitySynonym.filename)
            .filter(EntitySynonym.project_id == project_id)
            .distinct()
            .all()
        )
        synonym_filenames = {row[0] for row in synonym_filenames}

        # Responses with filenames correspond to responses that were specified
        # via training data files. For the moment, this is restricted to
        # Response Selector responses.

        responses_filenames = (
            self.query(Response.filename)
            .filter(Response.project_id == project_id)
            .distinct()
            .all()
        )

        responses_filenames = {row[0] for row in responses_filenames}

        return (
            training_data_filenames
            | responses_filenames
            | regex_filenames
            | lookup_filenames
            | synonym_filenames
        )

    def get_training_data_for_filename(
        self, project_id: Text, filename: Text
    ) -> List[Dict[Text, Any]]:
        """Get corresponding training data for a specific filename.

        Args:
            project_id: Project ID.
            filename: Filename to filter by.

        Returns:
            List of training data elements found for the file.
        """
        return self.get_training_data(project_id, filename=filename)[0]

    def assign_filename_based_on_intent(self, intent: Text, project_id: Text) -> Text:
        """Assign a filename based on the intent of an example.

        Args:
            intent: Intent the example has.
            project_id: Project the example belongs to.

        Returns:
            File which already stores examples of this intent or the oldest NLU file.
        """

        result = (
            self.query(TrainingData.filename)
            .filter(
                TrainingData.project_id == project_id, TrainingData.intent == intent
            )
            .first()
        )
        if result and result.filename:
            return result.filename
        else:
            return self.assign_filename(project_id)

    def assign_filename(self, project_id: Text) -> Text:
        """Finds the filename of the oldest `TrainingData` entry.

        Args:
            project_id: Project name which should be searched for intents.

        Returns:
            Path to oldest training data entry, or `rasa_x_config.default_nlu_filename`
            if no filename was found.
        """

        result = (
            self.query(TrainingData.filename)
            .filter(TrainingData.project_id == project_id)
            .order_by(TrainingData.id.asc())
            .first()
        )

        if result:
            file_path = result[0]
        else:
            project_directory = io_utils.get_project_directory()
            file_path = (
                project_directory
                / rasa_x_config.data_dir
                / rasa_x_config.default_nlu_filename
            )

        return str(file_path)

    def save_example(
        self,
        username: Text,
        project_id: Text,
        example: Dict[Text, Any],
        filename: Optional[Text] = None,
        dump_data: bool = True,
        add_example_items_to_domain: bool = True,
    ) -> Dict[Text, Any]:
        """Saves a training example or updates it if the example already exists.

        Args:
            username: User that creates the training example.
            project_id: Project ID of the example.
            example: Example to be created.
            filename: Filename of the example.
            dump_data: `True` if we want to dump into the file.
            add_example_items_to_domain: `True` if example items are added to the domain.

        Raises:
            TrainingDataCreationFailed: If the training example couldn't be created.

        Returns:
            Data of the created example.
        """
        existing_id = example.get("id")

        if not filename:
            filename = self.assign_filename_based_on_intent(
                example.get("intent"), project_id
            )

        if existing_id is not None:
            existing_example = self.get_training_example_by_id(existing_id)
        else:
            text_hash = common_utils.get_text_hash(example.get("text", ""))
            existing_example = self._get_example_by_hash(project_id, text_hash)

        existing_id = existing_example.id if existing_example else None

        # SQLAlchemy automatically updates the attributes of a mapped object
        # so we have to record the filename now for later use
        existing_filename = existing_example.filename if existing_example else None

        if existing_id is not None:
            # if this example already exists we update the existing one
            new_example = self.replace_example(
                {constants.USERNAME_KEY: username},
                project_id,
                example,
                existing_id,
                filename,
            )
        else:
            try:
                new_example_object = self._construct_valid_training_data_object(
                    example,
                    project_id,
                    filename,
                    username,
                    create_synonym_mappings=True,
                )
            except ValueError as e:
                raise TrainingDataCreationFailed(
                    "Failed to create training data. ",
                    "Creating synonym mapping failed. ",
                    f"Error: {e} ",
                )
            self.add(new_example_object)
            self.flush()  # flush to get the example id
            new_example = new_example_object.as_dict()

            log = self._set_log_in_training_data(new_example["hash"], True)
            if log:
                websocket_service.send_message(
                    websocket_service.Message(
                        websocket_service.MessageTopic.NLU,
                        name="log_marked_as_in_training_data",
                        data={"log_id": log.id},
                    )
                )

        if dump_data:
            # dump the desired or assigned filename, as well as that of the old
            # example that's been replaced (if present)
            nlu_files_to_dump = {filename}
            if existing_filename:
                nlu_files_to_dump.add(existing_filename)

            background_dump_service.add_nlu_changes(nlu_files_to_dump)

        if add_example_items_to_domain:
            self._add_intents_and_entities_to_domain(
                [example], project_id, username, dump_domain=True
            )

        return new_example

    def add_single_example(
        self, example: Dict[Text, Any], project_id: Text, user: Dict[Text, Any],
    ) -> Dict[Text, Any]:
        """Creates a single training data example.

        Creates a single training data example or updates it if an example
        with the same hash already exists.

        Args:
            example: Example data to be created
            project_id: Project id of the project that example should be added to.
            user: User that annotated the message.

        Raises:
            TrainingDataCreationFailed: If the training example couldn't be created.

        Returns:
            Data of the created example.
        """
        example = self.save_example(user[constants.USERNAME_KEY], project_id, example,)

        websocket_service.send_message(
            websocket_service.Message(
                websocket_service.MessageTopic.NLU,
                name="message_annotated",
                recipient=user[constants.USERNAME_KEY],
            )
        )
        return example

    def update_single_training_data_example(
        self,
        example_id: int,
        payload: Dict[Text, Any],
        project_id: Text,
        user: Dict[Text, Any],
    ) -> Dict[Text, Any]:
        """Updates a single training data example by `id`.

        Args:
            example_id: ID of an existing training data example.
            payload: Values to update.
            project_id: ID of the project to update the examples in.
            user: User that performs the operation.

        Raises:
            TrainingDataCreationFailed: If the example couldn't be replaced.
            TrainingDataNotFound: If the example couldn't be found by ID.

        Returns:
            Updated training data example.
        """
        import rasax.community.services.intent_service as intent_service

        existing_example = self.get_training_example_by_id(example_id)
        if not existing_example:
            raise TrainingDataNotFound(
                f"Training example with ID {example_id} not found."
            )

        mapped_to = payload.get("intent_mapped_to")
        if mapped_to:
            int_service = intent_service.IntentService(self.session)
            intent = {intent_service.INTENT_MAPPED_TO_KEY: mapped_to}
            int_service.update_temporary_intent(payload["intent"], intent, project_id)
            int_service.add_example_to_temporary_intent(
                payload["intent"], existing_example.hash, project_id
            )
        return self.replace_example(user, project_id, payload, example_id)

    def get_training_example_by_id(self, _id: int) -> Optional[TrainingData]:
        """Get `TrainingData` object for `_id`."""
        return self.query(TrainingData).filter(TrainingData.id == _id).first()

    def save_user_event_as_example(
        self, user: Dict[Text, Text], project_id: Text, event: Dict[Text, Any]
    ) -> Dict[Text, Any]:
        """Adds a training example based on a tracker event.

        Args:
            user: User of the training data.
            project_id: Project id of the new training example.
            event: The tracker event of the user message.

        Returns:
            If successful the stored example, otherwise `None`.
        """

        parse_data = event["parse_data"]
        example = {
            "intent": parse_data["intent"]["name"],
            "entities": parse_data.get("entities", []),
            "text": event.get("text"),
        }
        return self.save_example(user[constants.USERNAME_KEY], project_id, example)

    def replace_example(
        self,
        user: Dict[Text, Text],
        project_id: Text,
        example: Dict[Text, Any],
        example_id: int,
        filename: Optional[Text] = None,
    ) -> Optional[Dict[Text, Any]]:
        """Replace the contents/properties of an existing training data example.

        Args:
            user: User performing the changes.
            project_id: Project ID to set.
            example: NLU data dictionary.
            example_id: ID of training data example to perform changes on.
            filename: Filename to set.

        Raises: TrainingDataUpdateFailed: If training example couldn't be replaced.

        Returns:
            Modified training data object if `example_id` was valid, else `None`.
        """
        existing = self.get_training_example_by_id(example_id)

        if not existing:
            return None

        filename = filename or existing.filename

        # sqlalchemy automatically updates the attributes of a mapped object
        # so we have to record the filename now for later use
        existing_filename = existing.filename
        existing_hash = existing.hash
        try:
            updated = self._construct_valid_training_data_object(
                example,
                project_id,
                filename,
                user[constants.USERNAME_KEY],
                create_synonym_mappings=True,
            )
        except ValueError as e:
            raise TrainingDataUpdateFailed(
                "Failed to create training data. ",
                "Creating synonym mapping failed. ",
                f"Error: {e} ",
            )
        updated.id = existing.id

        self.merge(updated)
        background_dump_service.add_nlu_changes({filename, existing_filename})

        self._add_intents_and_entities_to_domain(
            [example], project_id, user[constants.USERNAME_KEY], dump_domain=True
        )

        # Maybe the hash has been modified. We need to take a
        # look at the message logs table to update the corresponding log
        # property: `in_training_data`.
        # The 'old' log is the log referred to by the pre-existing
        # hash. The 'new' one is the one referred by the updated hash. If
        # the hash hasn't changed, then we are simply modifying one log, and there
        # is no 'old' one.
        if existing_hash != updated.hash:
            # The 'old' log isn't referred by a training data item
            # anymore. Therefore, it is not included in the training data anymore.
            # Note that this only holds if training data item hashes are unique.
            self._set_log_in_training_data(existing_hash, False)

        # The 'new' log is in the training data
        self._set_log_in_training_data(updated.hash, True)

        return updated.as_dict()

    def _add_intents_and_entities_to_domain(
        self,
        examples: List[Dict[Text, Any]],
        project_id: Text,
        username: Text,
        dump_domain: bool = False,
    ) -> None:

        intents = [ex.get("intent") for ex in examples]
        entities = [e.get("entity") for ex in examples for e in ex.get("entities", [])]

        self._add_items_to_domain(
            entities, intents, project_id, username, dump_domain=dump_domain
        )

    def _set_log_in_training_data(
        self, message_hash: Text, in_training_data: bool
    ) -> Optional[MessageLog]:
        """Set an NLU log's `in_training_data` attribute.

        Args:
            message_hash: Text hash of the NLU log.
            in_training_data: Boolean value to set.

        Returns:
            When found, the NLU log that was modified.
        """
        from rasax.community.services.logs_service import LogsService

        log = LogsService(self.session).get_log_by_hash(message_hash)
        if log:
            log.in_training_data = in_training_data

        return log

    def add_all_intents_and_entities_to_domain(
        self, project_id: Text, username: Text
    ) -> None:
        """Adds intents and entities to domain from all training data."""

        intents = [i["intent"] for i in self.get_intents(project_id)]
        entities = [
            e["entity"]
            for e in self.get_entities(
                project_id, should_exclude_extractor_entities=True
            )
        ]
        self._add_items_to_domain(entities, intents, project_id, username)

    def _add_items_to_domain(
        self,
        entities: List[Text],
        intents: List[Text],
        project_id: Text,
        username: Text,
        dump_domain: bool = False,
    ):
        from rasax.community.services.domain_service import DomainService

        DomainService(self.session).add_items_to_domain(
            project_id,
            intents=intents,
            entities=entities,
            dump_data=dump_domain,
            origin="NLU training data",
        )

    def _get_writer_instance(self, filename: Text) -> TrainingDataWriter:
        """Return an instance of `TrainingDataWriter` that can write a certain training data file.

        Args:
            filename: Filename of training data file.

        Raises:
            ValueError: if the filename extension could not be recognized.

        Returns:
            Training data writer matching the file's format.
        """
        from rasa.shared.nlu.training_data.formats.markdown import MarkdownWriter
        from rasa.shared.nlu.training_data.formats.rasa import RasaWriter
        from rasa.shared.nlu.training_data.formats.rasa_yaml import RasaYAMLWriter

        if rasa_data.is_likely_markdown_file(filename):
            return MarkdownWriter()
        elif rasa_data.is_likely_yaml_file(filename):
            return RasaYAMLWriter()
        elif rasa_data.is_likely_json_file(filename):
            return RasaWriter()
        else:
            raise ValueError("Unknown file format for: '{filename}'")

    def dump_nlu_data(
        self, project: Text, files: Optional[Iterable[Text]] = None
    ) -> None:
        """Dump Rasa NLU data in database to file at `path`."""
        from rasa.shared.nlu.training_data.formats.markdown import MarkdownWriter

        if not files:
            files = self.get_all_filenames(project)

        for filename in files:
            logger.debug(f"Dumping NLU data to file '{filename}'.")

            writer = self._get_writer_instance(filename)

            # Markdown format needs to have the lookup table entries in a separate file
            include_lookup_table_entries = not isinstance(writer, MarkdownWriter)

            training_data = self.get_nlu_training_data_object(
                project,
                filename,
                should_include_lookup_table_entries=include_lookup_table_entries,
            )

            io_utils.create_path(filename)
            writer.dump(filename, training_data)

    def _get_example_by_hash(
        self, project_id: Text, _hash: Text
    ) -> Optional[TrainingData]:
        return (
            self.query(TrainingData)
            .filter(
                and_(TrainingData.project_id == project_id, TrainingData.hash == _hash)
            )
            .first()
        )

    def get_example_by_hash(
        self, project_id: Text, _hash: Text
    ) -> Optional[Dict[Text, Any]]:
        """Fetch a training example by its `_hash`.

        Args:
            project_id: Project ID associated with the example.
            _hash: MD5 hash of the example's `text` field.

        Returns:
            JSON dump of training example if it exists, else `None`.
        """
        result = self._get_example_by_hash(project_id, _hash)
        return result.as_dict() if result else None

    def delete_example(self, _id: int) -> bool:
        """Delete a training example by its ID.

        Args:
            _id: ID of the example to be deleted.

        Returns:
            `True` if the deletion was successful, else `False`.
        """
        result = self.get_training_example_by_id(_id)

        if result:
            self.delete(result)
            background_dump_service.add_nlu_changes([result.filename])

            log = self._set_log_in_training_data(result.hash, False)
            if log:
                websocket_service.send_message(
                    websocket_service.Message(
                        websocket_service.MessageTopic.NLU,
                        name="log_unmarked_as_in_training_data",
                        data=log.as_dict(),
                    )
                )

        return result is not None

    def delete_example_by_hash(self, project_id: Text, _hash: Text) -> bool:
        """Deletes an example based on its text hash.

        Args:
            project_id: Project id of the training example.
            _hash: Text hash of the training example.

        Returns:
            `True` if an example was deleted, otherwise `False`.
        """
        result = (
            self.query(TrainingData)
            .filter(
                and_(TrainingData.project_id == project_id, TrainingData.hash == _hash)
            )
            .all()
        )

        self.delete_all(result)
        self._set_log_in_training_data(_hash, False)

        return result

    def _bulk_save_entity_synonyms(
        self,
        project_id: Text,
        synonyms: Dict[Text, Text],
        filename: Optional[Text] = None,
    ) -> None:
        """Persists all entity synonyms and their mapped values. Argument `synonyms`
        must follow the format used by NLU training data synonyms:

        ```
        {
            "LA": "los angeles",
            "nyc": "new york",
            "ny": "new york",
            "manhattan": "new york"
        }
        ```

        Args:
            project_id: Current project ID.
            synonyms: Dictionary containing mapped values as dict keys, and entity
                synonyms as dict values (the order is reversed to allow many values
                to map to the same entity synonym).
            filename: File from where the synonyms were read.
        """

        entity_synonyms = defaultdict(list)
        for mapping_name, synonym_name in synonyms.items():
            entity_synonyms[synonym_name].append(mapping_name)

        for synonym_name, mapped_values in entity_synonyms.items():
            try:
                self.create_entity_synonym(
                    project_id, synonym_name, mapped_values, filename
                )
            except ValueError as e:
                # Bulk inserts should not fail in case one single item is invalid.
                # Instead it should just skip this item.
                cli_utils.raise_warning(str(e))

    def delete_additional_training_features(self, project_id: Text) -> None:
        """Deletes regex features, lookup tables and entity synonyms.

        Args:
            project_id: ID of the project.
        """
        self.query(RegexFeature).filter(RegexFeature.project_id == project_id).delete()
        self.query(LookupTable).filter(LookupTable.project_id == project_id).delete()
        self.query(EntitySynonym).filter(
            EntitySynonym.project_id == project_id
        ).delete()

    def save_additional_training_features(
        self,
        project_id: Text,
        training_data: nlu_data.TrainingData,
        filename: Optional[Text] = None,
    ) -> None:
        """Saves regex features, lookup tables and entity synonyms.

        Args:
            project_id: ID of the project.
            training_data: Training data that contains data that needs to be saved.
            filename: File from where the training data was read.
        """

        # Create new ones
        regex_features = [
            RegexFeature(
                name=r.get("name"),
                pattern=r.get("pattern"),
                project_id=project_id,
                filename=filename,
            )
            for r in training_data.regex_features
        ]

        lookup_tables = [
            LookupTable(
                name=table.get("name"),
                number_of_elements=len(table.get("elements", [])),
                elements=json.dumps(table.get("elements", [])),
                project_id=project_id,
                referencing_nlu_file=filename,
            )
            for table in training_data.lookup_tables
            # Ignore lookup tables that contain a filename as "elements"
            if isinstance(table.get("elements", []), list)
        ]

        # insert new entries
        self.bulk_save_objects(regex_features)
        self.bulk_save_objects(lookup_tables)
        self._bulk_save_entity_synonyms(
            project_id, training_data.entity_synonyms, filename
        )

        self.commit()

    def get_regex_features(
        self,
        project_id: Text,
        filename: Optional[Text] = None,
        offset: Optional[int] = None,
        limit: Optional[int] = None,
    ) -> common_utils.QueryResult:
        regex_features = self.query(RegexFeature).filter(
            RegexFeature.project_id == project_id
        )

        number_of_total_regexes = regex_features.count()
        if filename:
            regex_features = regex_features.filter(RegexFeature.filename == filename)

        regex_features = regex_features.order_by(RegexFeature.id.asc())
        regex_features = regex_features.offset(offset).limit(limit)
        regex_features = [r.as_dict() for r in regex_features.all()]
        return common_utils.QueryResult(regex_features, number_of_total_regexes)

    def _get_regex_feature_by_id(self, regex_id: int) -> RegexFeature:
        regex = self.query(RegexFeature).filter(RegexFeature.id == regex_id).first()

        if not regex:
            raise ValueError(f"Regex with id '{regex_id}' was not found")
        return regex

    def get_regex_feature_by_id(self, regex_id: int) -> Dict[Text, Union[int, Text]]:
        """Get regex feature by its unique id."""

        return self._get_regex_feature_by_id(regex_id).as_dict()

    def _get_regex_by_pattern(
        self, pattern: Text, project_id: Text
    ) -> Optional[Dict[Text, Union[int, Text]]]:
        regex = (
            self.query(RegexFeature)
            .filter(
                and_(
                    RegexFeature.pattern == pattern,
                    RegexFeature.project_id == project_id,
                )
            )
            .first()
        )
        if regex:
            return regex.as_dict()
        return None

    def create_regex_feature(
        self,
        regex_feature: Dict[Text, Text],
        project_id: Text,
        filename: Optional[Text] = None,
    ) -> Dict[Text, Union[int, Text]]:
        """Creates a new regex feature."""

        regex_with_same_pattern = self._get_regex_by_pattern(
            regex_feature["pattern"], project_id
        )
        if regex_with_same_pattern:
            raise ValueError(
                "Regex '{}' already has the same pattern '{}'.".format(
                    regex_with_same_pattern["name"], regex_feature["pattern"]
                )
            )
        filename = filename or self.assign_filename(project_id)
        new = RegexFeature(
            name=regex_feature["name"],
            pattern=regex_feature["pattern"],
            project_id=project_id,
            filename=filename,
        )
        self.add(new)
        self.flush()  # flush to get the example id

        background_dump_service.add_nlu_changes([filename])

        return new.as_dict()

    def update_regex_feature(
        self, regex_id: int, updated_regex: Dict[Text, Union[int, Text]]
    ) -> Dict[Text, Union[int, Text]]:
        """Update an existing regex."""

        existing = self._get_regex_feature_by_id(regex_id)
        existing.name = updated_regex["name"]
        existing.pattern = updated_regex["pattern"]

        background_dump_service.add_nlu_changes([existing.filename])

        return existing.as_dict()

    def delete_regex_feature(self, regex_id: int) -> None:
        """Delete regex feature by its id.

        Raises `ValueError` in case the the id does not exist.
        """
        to_delete = self._get_regex_feature_by_id(regex_id)
        self.delete(to_delete)
        background_dump_service.add_nlu_changes([to_delete.filename])

    @staticmethod
    def _rasa_reader_format_entity_synonyms(
        entity_synonyms: List[EntitySynonym],
    ) -> List[Dict[Text, Any]]:
        """Format entity synonyms into shape expected by `RasaReader`.

        `RasaReader` expects a list of dictionaries of the form

        {
            "synonyms": [
              "Chines",
              "chines",
              "Chinese"
            ],
            "value": "chinese"
        }
        """
        return [
            {
                "synonyms": [value.name for value in entity_synonym.synonym_values],
                "value": entity_synonym.name,
            }
            for entity_synonym in entity_synonyms
        ]

    def _entity_synonym_value_use_count(
        self, entity_synonym_value: EntitySynonymValue
    ) -> int:
        """Given a single synonym mapped value, calculate how many times
        its text value is used in the NLU training data.

        Args:
            entity_synonym_value: The mapped value on which to operate on.

        Returns:
            Number representing the number of times the text value of the
                mapped value is used in the NLU training data.
        """

        return (
            self.query(TrainingDataEntity)
            .filter(TrainingDataEntity.original_value == entity_synonym_value.name)
            .count()
        )

    def _entity_synonym_values_use_counts(
        self, entity_synonym: EntitySynonym
    ) -> Dict[Text, int]:
        """Given an entity synonym, calculate how many times each of its mapped values
        are used in the NLU training data.

        Args:
            entity_synonym: The entity synonym on which to operate on.

        Returns:
            A dictionary containing the use count of each mapped value, using
                the mapped value's ID as key.
        """

        example_counts = {}

        for entity_synonym_value in entity_synonym.synonym_values:
            example_counts[
                entity_synonym_value.id
            ] = self._entity_synonym_value_use_count(entity_synonym_value)

        return example_counts

    def _api_consumers_format_entity_synonyms(
        self, entity_synonyms: List[EntitySynonym]
    ) -> List[Dict[Text, Any]]:
        """Format entity synonyms into shape expected by consumers of the Rasa X API.

        Args:
            entity_synonyms: List of entity synonyms to format.

        Returns:
            List of entity synonyms in a JSON-like format.
        """

        return [
            entity_synonym.as_dict(
                self._entity_synonym_values_use_counts(entity_synonym)
            )
            for entity_synonym in entity_synonyms
        ]

    def get_entity_synonyms(
        self,
        project_id: Text,
        text_query: Optional[Text] = None,
        filename: Optional[Text] = None,
        nlu_format: bool = True,
    ) -> List[Dict[Text, Any]]:
        """Get a JSON-like representation of all entity synonyms, and their respective
        mapped values.

        Args:
            project_id: Filter by project ID.
            text_query: Only return synonymms that contain this text
            filename: Filter by filename on which the entity synonyms were defined.
            nlu_format: If `True`, return results in a format compatible with
                `RasaReader`. If `False`, use the format defined by the Rasa X API.

        Returns:
            Entity synonyms and their properties, including their mapped values (and
            their properties).
        """

        entity_synonyms = self.query(EntitySynonym).filter(
            EntitySynonym.project_id == project_id
        )

        if filename:
            entity_synonyms = entity_synonyms.filter(EntitySynonym.filename == filename)
        if text_query:
            entity_synonyms = entity_synonyms.filter(
                EntitySynonym.name.ilike(f"%{text_query}%")
            )

        entity_synonyms = entity_synonyms.order_by(EntitySynonym.id.asc()).all()
        if nlu_format:
            return self._rasa_reader_format_entity_synonyms(entity_synonyms)

        return self._api_consumers_format_entity_synonyms(entity_synonyms)

    def _get_entity_synonym(
        self, project_id: Text, entity_synonym_id: int
    ) -> Optional[EntitySynonym]:
        """Get an entity synonym.

        Args:
            project_id: Filter by project ID.
            entity_synonym_id: The ID of the entity synonym.

        Returns:
            An `EntitySynonym` if it was found, or `None` otherwise.
        """

        return (
            self.query(EntitySynonym)
            .filter(
                EntitySynonym.project_id == project_id,
                EntitySynonym.id == entity_synonym_id,
            )
            .one_or_none()
        )

    def get_entity_synonym(
        self, project_id: Text, entity_synonym_id: int
    ) -> Optional[Dict[Text, Any]]:
        """Get a JSON-like representation of an entity synonym, with its mapped values.

        Args:
            project_id: Filter by project ID.
            entity_synonym_id: The ID of the entity synonym.

        Returns:
            A dictionary representing the entity synonym if it was found, or `None`
            otherwise.
        """

        entity_synonym = self._get_entity_synonym(project_id, entity_synonym_id)

        if entity_synonym:
            return entity_synonym.as_dict(
                self._entity_synonym_values_use_counts(entity_synonym)
            )

        return None

    def create_entity_synonym(
        self,
        project_id: Text,
        synonym_name: Text,
        mapped_values: Set[Text],
        filename: Optional[Text] = None,
    ) -> Optional[EntitySynonym]:
        """Creates a new entity synonym with one or more mapped values.

        Args:
            project_id: Project ID.
            synonym_name: Name of the entity synonym.
            mapped_values: Set of mapped values to create.
            filename: Name of the training data file which stored this synonym.

        Returns:
            `None` if an `EntitySynonym` with that name already exists,
            otherwise an `EntitySynonym` object.

        Raises:
            ValueError: If the mapped values set is empty or if the entity synonym's
            name is contained in the mapped values set.
        """
        existing = (
            self.query(EntitySynonym)
            .filter(
                EntitySynonym.project_id == project_id,
                EntitySynonym.name == synonym_name,
            )
            .first()
        )
        if existing:
            return None

        if not filename:
            filename = self.assign_filename(project_id)

        entity_synonym = EntitySynonym(
            name=synonym_name, project_id=project_id, filename=filename
        )
        self.add(entity_synonym)
        self.flush()

        self.add_entity_synonym_mapped_values(
            project_id, entity_synonym.id, mapped_values
        )

        return entity_synonym

    def update_entity_synonym(
        self, project_id: Text, entity_synonym_id: int, new_value: Text
    ) -> Optional[EntitySynonym]:
        """Update the value ("name") of an entity synonym.

        Args:
            project_id: Filter by project ID.
            entity_synonym_id: The ID of the entity synonym.
            new_value: New value to set.

        Returns:
            An `EntitySynonym` object if the entity synonym was updated successfully and
            `None` if another entity synonym with name == `new_value` already exists.

        Raises:
            ValueError: If the entity synonym with ID `entity_synonym_id` could not be
            found.
        """

        entity_synonym = self._get_entity_synonym(project_id, entity_synonym_id)
        if not entity_synonym:
            raise ValueError(
                f"EntitySynonym with ID '{entity_synonym_id}' does not exist."
            )

        existing = (
            self.query(EntitySynonym)
            .filter(
                EntitySynonym.project_id == project_id, EntitySynonym.name == new_value
            )
            .first()
        )

        if existing:
            return None

        entity_synonym.name = new_value

        if entity_synonym.filename:
            background_dump_service.add_nlu_changes([entity_synonym.filename])
        return entity_synonym

    def delete_entity_synonym(self, project_id: Text, entity_synonym_id: int) -> bool:
        """Deletes an entity synonym.

        Args:
            project_id: Filter by project ID.
            entity_synonym_id: The ID of the entity synonym.

        Returns:
            `True` if the entity synonym was found and deleted, and `False` otherwise.
        """
        entity_synonym = self._get_entity_synonym(project_id, entity_synonym_id)
        if not entity_synonym:
            return False

        self.delete(entity_synonym)

        if entity_synonym.filename:
            background_dump_service.add_nlu_changes([entity_synonym.filename])
        return True

    def delete_entity_synonym_mapped_value(
        self, project_id: Text, entity_synonym_id: int, mapping_id: int
    ) -> bool:
        """Delete a value mapped to an entity synonym.

        If the entity synonym no longer has any mapped values after deleting the
        specified mapped value, it will be deleted as well.
        This also deletes the synonym reference from `TrainingDataEntity`s using this
        mapped value.

        Args:
            project_id: Filter by project ID.
            entity_synonym_id: The ID of the entity synonym.
            mapping_id: ID of the mapped value to delete.

        Returns:
            `True` if the mapped value was deleted successfully and `False` if no such
            mapped value was found.

        Raises:
            ValueError: If the entity synonym with ID `entity_synonym_id` could not be
            found.
        """
        entity_synonym = self._get_entity_synonym(project_id, entity_synonym_id)
        if not entity_synonym:
            raise ValueError(
                f"EntitySynonym with ID '{entity_synonym_id}' does not exist."
            )

        synonym_values_count = len(entity_synonym.synonym_values)
        to_delete = (
            self.query(EntitySynonymValue)
            .filter(
                EntitySynonymValue.entity_synonym_id == entity_synonym.id,
                EntitySynonymValue.id == mapping_id,
            )
            .first()
        )

        if not to_delete:
            return False
        self.delete(to_delete)

        if synonym_values_count == 1:
            # The entity synonym no longer has any mapped values. Delete it.
            self.delete(entity_synonym)
        else:
            self._remove_synonym_reference_to_entities(
                entity_synonym_id, to_delete.name
            )

        if entity_synonym.filename:
            background_dump_service.add_nlu_changes([entity_synonym.filename])
        return True

    def _get_entity_synonym_value(
        self, project_id: Text, entity_synonym_id: int, mapping_id: int
    ) -> Optional[EntitySynonymValue]:
        """Gets a mapped value for an entity synonym.

        Args:
            project_id: Filter by project ID.
            entity_synonym_id: The ID of the entity synonym.
            mapping_id: The ID of the mapped value.

        Returns:
            An `EntitySynonymValue` object if it was found, or `None` otherwise.

        Raises:
            ValueError: If the entity synonym with ID `entity_synonym_id` could not be
            found.
        """
        entity_synonym = self.get_entity_synonym(project_id, entity_synonym_id)
        if not entity_synonym:
            raise ValueError(
                f"EntitySynonym with ID '{entity_synonym_id}' does not exist."
            )

        return (
            self.query(EntitySynonymValue)
            .filter(
                EntitySynonymValue.entity_synonym_id == entity_synonym["id"],
                EntitySynonymValue.id == mapping_id,
            )
            .one_or_none()
        )

    def _remove_synonym_reference_to_entities(
        self, synonym_id: int, mapped_value: Text
    ) -> None:
        referencing_entities = (
            self.query(TrainingDataEntity)
            .filter(
                TrainingDataEntity.entity_synonym_id == synonym_id,
                TrainingDataEntity.original_value == mapped_value,
            )
            .all()
        )
        for entity in referencing_entities:
            entity.entity_synonym_id = None

    def _get_entity_synonym_value_by_name(
        self, project_id: Text, entity_synonym_id: int, mapped_value: Text
    ) -> Optional[EntitySynonymValue]:
        """Get a mapped value for an entity synonym, searching by its name.

        Args:
            project_id: Filter by project ID.
            entity_synonym_id: The ID of the entity synonym.
            mapped_value: Filter by `.name` property of the `EntitySynonymValue`.

        Returns:
            An `EntitySynonymValue` object if it was found, or `None` otherwise.

        Raises:
            ValueError: If the entity synonym with ID `entity_synonym_id` could not be
            found.
        """

        entity_synonym = self.get_entity_synonym(project_id, entity_synonym_id)
        if not entity_synonym:
            raise ValueError(
                f"EntitySynonym with ID '{entity_synonym_id}' does not exist."
            )

        return (
            self.query(EntitySynonymValue)
            .filter(
                EntitySynonymValue.entity_synonym_id == entity_synonym["id"],
                EntitySynonymValue.name == mapped_value,
            )
            .one_or_none()
        )

    def get_synonym_by_mapped_value(
        self, mapped_value: Text, project_id: Text
    ) -> Optional[Dict[Text, Any]]:
        """Get synonyms by a mapped value.

        Args:
            mapped_value: Value which should be used to query the synonyms.
            project_id: Project the synonym should belong to.

        Returns:
            Matching entity synonyms (should always be of length 1).
        """
        synonym = (
            self.query(EntitySynonymValue)
            .filter(
                EntitySynonymValue.entity_synonym.has(
                    EntitySynonym.project_id == project_id
                ),
                EntitySynonymValue.name == mapped_value,
            )
            .first()
        )

        return synonym.entity_synonym.as_dict() if synonym else None

    def add_entity_synonym_mapped_values(
        self, project_id: Text, entity_synonym_id: int, mapped_values: List[Text]
    ) -> Optional[List[Dict[Text, Any]]]:
        """Map new values to an existing entity synonym.

        Args:
            project_id: Project ID.
            entity_synonym_id: ID of the entity synonym.
            mapped_values: Values to map to the entity synonym.

        Returns:
            List of all the newly mapped values, or `None` if any of the values was
            already mapped.

        Raises:
            ValueError: If the entity synonym with ID `entity_synonym_id` could not be
            found, or if mapped values set is empty, or if any of the mapped values'
            text is equal to the text value of the entity synonym.
        """

        entity_synonym = self._get_entity_synonym(project_id, entity_synonym_id)
        if not entity_synonym:
            raise ValueError(
                f"EntitySynonym with ID '{entity_synonym_id}' does not exist."
            )

        mapped_values_without_synonyms_value = [
            value for value in mapped_values if value != entity_synonym.name
        ]
        if len(mapped_values_without_synonyms_value) < len(mapped_values):
            cli_utils.raise_warning(
                f"Value to map '{entity_synonym.name}' is equal to the entity "
                f"synonym's value."
            )

        unique_mapped_values = common_utils.deduplicate_preserving_order(
            mapped_values_without_synonyms_value
        )
        existing_mappings = (
            self.query(EntitySynonymValue)
            .filter(
                EntitySynonymValue.entity_synonym_id == entity_synonym_id,
                EntitySynonymValue.name.in_(unique_mapped_values),
            )
            .all()
        )

        for existing in existing_mappings:
            unique_mapped_values.remove(existing.name)

        number_of_already_mapped_values = len(
            mapped_values_without_synonyms_value
        ) - len(unique_mapped_values)
        if number_of_already_mapped_values > 0:
            cli_utils.raise_warning(
                f"Skipped adding {number_of_already_mapped_values} because you are "
                f"trying to map values which have been previously mapped or are "
                f"repeated."
            )
        if not unique_mapped_values and number_of_already_mapped_values > 0:
            raise ValueError("Synonym values are already mapped.")

        if not unique_mapped_values:
            raise ValueError("At least one value to map must be specified.")

        entity_synonym_values = self._add_entity_synonym_values(
            entity_synonym, unique_mapped_values
        )
        self.flush()

        if entity_synonym.filename:
            background_dump_service.add_nlu_changes([entity_synonym.filename])

        return [
            value.as_dict(self._entity_synonym_value_use_count(value))
            for value in entity_synonym_values
        ]

    def _add_entity_synonym_values(
        self, entity_synonym: EntitySynonym, mapped_values: List[Text]
    ) -> List[EntitySynonymValue]:
        """Adds `EntitySynonymValue` to an existing `EntitySynonym` in the database.

        Args:
            entity_synonym: `EntitySynonym` which has some mapped values.
            mapped_values: Mapped values of a synonym which should be added to
                the database.

        Returns:
            Created `EntitySynonymValue`s.
        """

        entity_synonym_values = []
        for mapped_value in mapped_values:
            entity_synonym_value = EntitySynonymValue(
                entity_synonym_id=entity_synonym.id, name=mapped_value
            )

            self.add(entity_synonym_value)
            entity_synonym_values.append(entity_synonym_value)

            self._add_synonym_reference_to_entities(entity_synonym.id, mapped_value)

        return entity_synonym_values

    def _add_synonym_reference_to_entities(
        self, entity_synonym_id: int, mapped_value: Text
    ) -> None:
        """Add the synonym id to entities which use a mapped value of
           this synonym.

        Args:
            entity_synonym_id: Id of the synonym.
            mapped_value: Value which is used to search the entities in order to see
                whether they are using this synonym.
        """

        entities_with_this_text = (
            self.query(TrainingDataEntity)
            .filter(TrainingDataEntity.original_value == mapped_value)
            .all()
        )
        for entity in entities_with_this_text:  # type: TrainingDataEntity
            entity.entity_synonym_id = entity_synonym_id

    def update_entity_synonym_mapped_value(
        self, project_id: Text, entity_synonym_id: int, mapping_id: int, new_value: Text
    ) -> Optional[EntitySynonymValue]:
        """Update the text value of a value mapped to an entity synonym.

        Args:
            project_id: Filter by project ID.
            entity_synonym_id: The ID of the entity synonym.
            mapping_id: ID of the mapped value to update.
            new_value: The new value to set.

        Returns:
            An `EntitySynonymValue` object if the mapped value was updated successfully
            and `None` if another mapped value with the same value already existed.

        Raises:
            ValueError: If the entity synonym with ID `entity_synonym_id` could not be
            found, or if the entity synonym mapped value with value `mapped_value` could
            not be found, or if the new value to map is equal to the entity synonym's
            value.
        """

        entity_synonym_value = self._get_entity_synonym_value(
            project_id, entity_synonym_id, mapping_id
        )

        if not entity_synonym_value:
            raise ValueError(
                f"EntitySynonymValue with ID '{mapping_id}' does not exist."
            )

        if new_value == entity_synonym_value.entity_synonym.name:
            raise ValueError(
                "Value to map '{}' is equal to the entity synonym's value.".format(
                    new_value
                )
            )

        repeated = self._get_entity_synonym_value_by_name(
            project_id, entity_synonym_id, new_value
        )

        if repeated:
            return None

        self._delete_mapped_synonym_value_from_entities(entity_synonym_id)

        # Add new mapped values to entities
        self._add_synonym_reference_to_entities(entity_synonym_id, new_value)

        entity_synonym_value.name = new_value

        entity_synonym = self._get_entity_synonym(project_id, entity_synonym_id)
        if entity_synonym and entity_synonym.filename:
            background_dump_service.add_nlu_changes([entity_synonym.filename])

        return entity_synonym_value

    def _delete_mapped_synonym_value_from_entities(
        self, entity_synonym_id: int
    ) -> None:
        entities_with_mapped_value = (
            self.query(TrainingDataEntity)
            .filter(TrainingDataEntity.entity_synonym_id == entity_synonym_id)
            .all()
        )
        for entity in entities_with_mapped_value:
            entity.entity_synonym_id = None

    def get_lookup_tables(
        self,
        project_id: Text,
        filename: Optional[Text] = None,
        include_filenames: bool = False,
    ) -> List[Dict[Text, Any]]:
        """Get all lookup tables for a certain project / filename.

        Args:
            project_id: id of the project of which the lookup tables should be returned
            filename: returns only lookup tables with a certain filename
            include_filenames: `True` if the filename should be included in the result

        Returns:
            All lookup tables matching the given conditions.
        """

        lookup_tables = self.query(LookupTable).filter(
            LookupTable.project_id == project_id
        )

        if filename:
            lookup_tables = lookup_tables.filter(
                LookupTable.referencing_nlu_file == filename
            )
        lookup_tables = lookup_tables.order_by(LookupTable.id.asc())

        return [r.as_dict(include_filenames) for r in lookup_tables.all()]

    def get_lookup_tables_with_elements(
        self, project_id: Text, filename: Optional[Text] = None
    ) -> List[Dict[Text, Union[Text, List[Text]]]]:
        """Get lookup tables including all their elements.

        While `get_lookup_table` does not return the lookup table elements, this method
        returns the `LookupTables` includes all of their elements.

        Args:
            project_id: the id of the project this data belongs to.
            filename: returns only lookup tables with a certain filename.

        Returns:
            the lookup tables including all their elements.
        """
        lookup_tables = self.query(LookupTable).filter(
            LookupTable.project_id == project_id
        )

        if filename:
            lookup_tables = lookup_tables.filter(
                LookupTable.referencing_nlu_file == filename
            )

        lookup_tables = lookup_tables.order_by(LookupTable.id.asc()).all()

        return [
            {"name": table.name, "elements": json.loads(table.elements)}
            for table in lookup_tables
        ]

    def save_lookup_table(
        self,
        filename: Text,
        content: Text,
        project_id: Text,
        nlu_filename: Text = None,
        dump_data: bool = True,
    ) -> Dict[Text, Union[Text, int]]:
        """Store a lookup table which was uploaded as file.

        Args:
            filename: name of the file
            content: content of the lookup tables separated by newlines
            project_id: the id of the project this data belongs to
            nlu_filename: name of the file (if any) that references the lookup table
            dump_data: `True` if lookup table should be saved as file

        Returns:
            created lookup table.
        """

        # that's the NLU file which references this lookup table file
        if not nlu_filename:
            nlu_filename = self.assign_filename(project_id)

        elements = content.splitlines()
        new = LookupTable(
            project_id=project_id,
            name=filename,
            referencing_nlu_file=nlu_filename,
            elements=json.dumps(elements),
            number_of_elements=len(elements),
        )
        self.add(new)
        # Flush to assign id
        self.flush()

        if dump_data:
            background_dump_service.add_lookup_table_change(
                new.id, referencing_nlu_file=new.referencing_nlu_file
            )

        return new.as_dict()

    def get_lookup_table(self, lookup_table_id: int) -> Text:
        """Get a lookup table by its id.

        Args:
            lookup_table_id: the id of the lookup table which should be retrieved

        Returns:
            The content of the lookup table split by newlines.

        Raises:
            ValueError: raised if no lookup table with this id exists
        """

        item = self._get_lookup_table_by_id(lookup_table_id)
        return self._format_lookup_table_elements_as_text(item)

    @staticmethod
    def _format_lookup_table_elements_as_text(lookup_table: LookupTable) -> Text:
        return "\n".join(json.loads(lookup_table.elements))

    def dump_lookup_tables(self, lookup_table_ids: Set[int]) -> None:
        """Dump lookup tables with the given IDs to disk.

        Args:
            lookup_table_ids: The IDs of the lookup tables which should be dumped.
        """
        for _id in lookup_table_ids:
            self._dump_lookup_table_content(_id)

    def _dump_lookup_table_content(self, lookup_table_id: int) -> None:
        lookup_table = self._get_lookup_table_by_id(lookup_table_id)
        formatted = self._format_lookup_table_elements_as_text(lookup_table)
        io_utils.write_file(lookup_table.absolute_file_path, formatted)

    def _get_lookup_table_by_id(self, lookup_table_id: int) -> LookupTable:
        item = self.query(LookupTable).filter(LookupTable.id == lookup_table_id).first()

        if not item:
            raise ValueError(
                f"Lookup table with id '{lookup_table_id}' does not exist."
            )
        return item

    def delete_lookup_table(self, lookup_table_id: int) -> None:
        """Delete a lookup table by its id.

        Args:
            lookup_table_id: the id of the lookup table to delete.

        Raises:
            ValueError: raised if no lookup table with this id exists.
        """

        to_delete = self._get_lookup_table_by_id(lookup_table_id)
        self.delete(to_delete)

        self._delete_lookup_table_on_file_system(to_delete.absolute_file_path)

    @staticmethod
    def _delete_lookup_table_on_file_system(lookup_table_path: Text) -> None:
        if (
            io_utils.should_dump()
            and os.path.exists(lookup_table_path)
            and os.path.isfile(lookup_table_path)
        ):
            logger.debug(f"Deleting lookup table file '{lookup_table_path}'.")
            os.remove(lookup_table_path)

    def get_entities(
        self, project_id: Text, should_exclude_extractor_entities: bool = False
    ) -> List[Dict[Text, Text]]:
        """Fetch entities in training data.

        Exclude entities that have an `extractor` attribute if
        `should_exclude_extractor_entities` is True.
        """

        query = TrainingData.project_id == project_id
        if should_exclude_extractor_entities:
            query = and_(query, TrainingDataEntity.extractor.is_(None))

        entities = (
            self.query(TrainingDataEntity.entity)
            .join(TrainingData)
            .filter(query)
            .distinct()
            .all()
        )

        return [{"entity": e} for e, in entities]

    def get_entity_roles(self, project_id: Text) -> List[Dict[Text, Text]]:
        """Fetch unique entity roles from training data."""
        roles = (
            self.query(TrainingDataEntity.role)
            .join(TrainingData)
            .filter(
                and_(
                    TrainingData.project_id == project_id,
                    TrainingDataEntity.role.isnot(None),
                )
            )
            .distinct()
            .all()
        )

        return [{"role": r} for r, in roles]

    def get_entity_groups(self, project_id: Text) -> List[Dict[Text, Text]]:
        """Fetch unique entity roles from training data."""
        groups = (
            self.query(TrainingDataEntity.group)
            .join(TrainingData)
            .filter(
                and_(
                    TrainingData.project_id == project_id,
                    TrainingDataEntity.group.isnot(None),
                )
            )
            .distinct()
            .all()
        )

        return [{"group": r} for r, in groups]

    def get_intents(
        self, project_id: Text, include_example_hashes: bool = True
    ) -> List[Dict[Text, Union[Text, List[Text]]]]:
        """Returns list of intents which appear in the training data.

        Args:
            project_id: Project id of the training data.
            include_example_hashes: If `True` include the hashes of the examples for
                                    each intent.

        Returns:
            List of intent objects which appear in the training data including
            the hashes of the related examples.
        """

        from rasax.community.services.intent_service import INTENT_EXAMPLES_KEY

        intents = (
            self.query(TrainingData.intent)
            .filter(TrainingData.project_id == project_id)
            .distinct()
            .all()
        )

        results = []
        for (i,) in intents:
            intent = {"intent": i}
            if include_example_hashes:
                intent[INTENT_EXAMPLES_KEY] = self._get_example_hashes_for_intent(i)

            results.append(intent)

        return results

    def _get_example_hashes_for_intent(self, intent: Text) -> List[Text]:
        example_hashes = (
            self.query(TrainingData.hash)
            .filter(TrainingData.intent == intent)
            .distinct()
            .all()
        )

        return [e for (e,) in example_hashes]

    def delete_data(self) -> None:
        """Deletes all examples."""
        old_training_data = self.query(TrainingData).all()
        self.delete_all(old_training_data)

    def replace_data(
        self,
        project_id: Text,
        data: Union[Text, Dict[Text, Any]],
        data_format: Text,
        username: Optional[Text] = None,
        filename: Optional[Text] = None,
        dump_data: bool = True,
    ) -> None:
        """Deletes all examples and adds examples from `data`.

        Args:
            project_id: Project ID.
            data: NLU data to insert.
            data_format: Format of data received.
            username: Username of user inserting the data.
            filename: File name of where data was read from.
            dump_data: If `True`, schedule dumping of NLU data to files after inserting.
        """
        from rasax.community.services.nlg_service import NlgService

        nlg_service = NlgService(self.session)

        # Delete all NLU training data first. Note that these steps are identical to the
        # ones followed in `initialise.inject_nlu_data`. The only difference is that
        # here we are receiving the data directly, not reading it from files.
        self.delete_data()
        self.delete_additional_training_features(project_id)
        nlg_service.delete_all_responses(True)

        self.save_bulk_data(
            project_id, data, data_format, username, filename, dump_data
        )

    def _save_bulk_data(
        self,
        examples: List[Dict[Text, Any]],
        username: Optional[Text],
        filename: Optional[Text],
        project_id: Text,
    ) -> None:
        objects = [
            self._construct_valid_training_data_object(
                e, project_id, filename, username, create_synonym_mappings=False
            )
            for e in examples
        ]
        self.add_all(objects)

    def _update_in_training_data_column_for_logs(self) -> None:
        """Bulk update the `in_training_data` column for all `MessageLog`s."""
        from rasax.community.services.logs_service import LogsService

        LogsService(self.session).bulk_update_in_training_data_column()

    def save_bulk_data_from_files(
        self, nlu_files: Union[List[Text], Set[Text]], project_id: Text, username: Text
    ) -> int:
        """Saves NLU data from `nlu_files` to database. It also replaces
        the already existing regex features, lookup tables and entity synonyms.

        Args:
            nlu_files: NLU files the data from which needs to be saved.
            project_id: ID of the project.
            username: Name of the user.

        Returns:
            The number of saved training examples.
        """

        training_examples_count = 0

        for data, path in _read_data(list(nlu_files)):
            logger.debug(f"Injecting NLU data from file '{path}' to database.")

            data_format = loading.guess_format(path)
            if data_format not in {
                rasa.shared.nlu.training_data.loading.RASA,
                rasa.shared.nlu.training_data.loading.MARKDOWN,
                rasa.shared.nlu.training_data.loading.RASA_YAML,
            }:
                logger.debug(
                    f"Invalid data format for file '{path}'. "
                    f"Accepting '{rasa.shared.nlu.training_data.loading.RASA_YAML}', "
                    f"'{rasa.shared.nlu.training_data.loading.RASA}' "
                    f"and '{rasa.shared.nlu.training_data.loading.MARKDOWN}' but found "
                    f"'{data_format}'."
                )
                continue

            additional_data = self.save_bulk_data(
                project_id,
                data,
                data_format,
                username=username,
                filename=path,
                dump_data=False,
                add_data_items_to_domain=False,
            )

            if additional_data:
                training_examples_count += len(additional_data.training_examples)

        self.add_all_intents_and_entities_to_domain(project_id, username)

        return training_examples_count

    def _save_referenced_lookup_tables(
        self, lookup_tables: List[Dict], project_id: Text, filename: Text
    ) -> None:
        """Tries to load and save lookup tables which were referenced in a NLU file."""
        for lookup_table in lookup_tables:
            elements = lookup_table.get("elements")

            if not isinstance(elements, str):
                continue  # it's an inline lookup table which was already saved

            lookup_table_path = io_utils.get_project_directory() / elements
            if lookup_table_path.is_file():
                logger.debug(f"Injecting referenced lookup table '{elements}'.")
                content = io_utils.read_file(str(lookup_table_path))
                self.save_lookup_table(
                    elements,
                    content,
                    project_id,
                    nlu_filename=filename,
                    dump_data=False,
                )

    @staticmethod
    def training_data_to_dict(training_data: NluTrainingData,) -> OrderedDict:
        """Represents NLU training data to a dict/list structure ready to be serialized as YAML.

        Args:
            training_data: `TrainingData` to convert.

        Returns:
            `OrderedDict` containing all training data.
        """
        from rasa.shared.nlu.training_data.formats.rasa_yaml import RasaYAMLWriter

        yaml_data = RasaYAMLWriter.training_data_to_dict(training_data)
        return yaml_data if yaml_data else OrderedDict([(KEY_NLU, [])])

    @staticmethod
    def _combine_response_keys_and_intents(training_data: NluTrainingData) -> None:
        """Combines response keys and intents in `training_data`.

        NLU training examples with `intent_response_key` attributes are
        modified such that the example intent is merged with the response key
        in the format 'intent/response_key'.

        Example:

            {
                'intent': 'faq',
                'intent_response_key': 'faq/ask_howdoing',
                'text': 'how are you?'
            }

            becomes

            {
                'intent': 'faq/ask_howdoing',
                'intent_response_key': 'faq/ask_howdoing',
                'text': 'how are you?'
            }

        Args:
            training_data: `NluTrainingData` to modify.
        """
        for message in training_data.training_examples:
            response_key = message.get(INTENT_RESPONSE_KEY)
            intent = message.get(INTENT)
            if response_key is not None and intent is not None:
                message.set(INTENT, response_key)

    def _save_training_data_responses(
        self,
        project_id: Text,
        username: Optional[Text],
        filename: Text,
        training_data: NluTrainingData,
    ) -> None:
        """Store responses contained inside a OSS `TrainingData` object in the database.

        Args:
            project_id: Project ID.
            username: Name of user inserting the responses.
            filename: Filename of file containing the training data object.
            training_data: Training data containing responses.
        """
        from rasax.community.services.nlg_service import NlgService

        nlg_service = NlgService(self.session)

        for name, response_list in training_data.responses.items():
            for response in response_list:
                response[constants.RESPONSE_NAME_KEY] = name
                nlg_service.save_response(
                    response, username, project_id=project_id, filename=filename
                )

    def save_bulk_data(
        self,
        project_id: Text,
        data: Union[Text, Dict[Text, Any]],
        data_format: Text,
        username: Optional[Text] = None,
        filename: Optional[Text] = None,
        dump_data: bool = True,
        add_data_items_to_domain: bool = True,
    ) -> Optional[NluTrainingData]:
        """Adds examples from `data` in YAML, json or markdown format."""

        if not filename:
            filename = self.assign_filename(project_id)

        if data_format == rasa.shared.nlu.training_data.loading.RASA:
            if isinstance(data, str):
                data = json.loads(data)
            training_data = RasaReader().read_from_json(data)
        elif data_format == rasa.shared.nlu.training_data.loading.MARKDOWN:
            if isinstance(data, (bytes, bytearray)):
                data = data.decode(io_utils.DEFAULT_ENCODING)
            training_data = MarkdownReader().reads(data)
        elif data_format == rasa.shared.nlu.training_data.loading.RASA_YAML:
            if isinstance(data, (bytes, bytearray)):
                data = data.decode(io_utils.DEFAULT_ENCODING)
            training_data = RasaYAMLReader().reads(data)
        else:
            return None

        # support retrieval intents by merging intents with response keys
        self._combine_response_keys_and_intents(training_data)

        # save regex features, entity synonyms and lookup tables if present
        self.save_additional_training_features(project_id, training_data, filename)

        # save referenced lookup tables
        self._save_referenced_lookup_tables(
            training_data.lookup_tables, project_id, filename
        )

        # save training examples
        examples = [m.as_dict() for m in training_data.training_examples]
        self._save_bulk_data(examples, username, filename, project_id)

        # Save responses contained in training data
        self._save_training_data_responses(
            project_id, username, filename, training_data
        )

        if dump_data:
            background_dump_service.add_nlu_changes([filename])

        if add_data_items_to_domain:
            self._add_intents_and_entities_to_domain(
                examples, project_id, username, dump_domain=dump_data
            )

        self._update_in_training_data_column_for_logs()

        return training_data

    def update_intent(
        self, new_intent: Text, example_hashes: List[Text], project_id: Text
    ) -> None:
        """Updates the intent of training examples.

        Args:
            new_intent: Name of the new intent.
            example_hashes: Hashes of the training examples which should be
                            updated.
            project_id: Project id of the training data.
        """

        examples = (
            self.query(TrainingData)
            .filter(
                and_(
                    TrainingData.project_id == project_id,
                    TrainingData.hash.in_(example_hashes),
                )
            )
            .all()
        )

        for e in examples:
            e.intent = new_intent

    def get_nlu_training_data_object(
        self,
        project_id: Text,
        filename: Optional[Text] = None,
        should_include_lookup_table_entries: bool = False,
    ) -> NluTrainingData:
        """Get an NLU `TrainingData` object from training data stored in the database.

        Args:
            filename: file name associated with the training data.
            project_id: Project id of the training data.
            should_include_lookup_table_entries: `True` includes the elements of the lookup
                tables in the data, `False` only adds the file links of the lookup
                tables.
        """
        from rasax.community.services.nlg_service import NlgService

        nlg_service = NlgService(self.session)
        combined_nlu_data = self.create_formatted_training_data(
            project_id, filename, should_include_lookup_table_entries
        )
        training_data = RasaReader().read_from_json(combined_nlu_data)

        # FIXME
        # Unfortunately, RasaReader.read_from_json() does not know how to read
        # Response Selector responses. There's two options here:
        #
        #  1) Update it so that it knows how to read those. Updating the JSON
        #  schema validation will be necessary.
        #
        #  2) Serialize `combined_nlu_data` to YAML, and then read it using
        #  `RasaYAMLReader`. This could get very slow though, and generally
        #  does not sound like a good idea.
        #
        # The current workaround is just setting the `response` property
        # *after* the `TrainingData` object has been initialized, and then
        # calling its private `_fill_response_phrases` method, which is called
        # at the end of the initializer. This works assuming we do not add more
        # logic to the initializer meanwhile.
        training_data.responses = nlg_service.get_training_data_responses(
            project_id, filename
        )
        training_data._fill_response_phrases()  # Private method call

        return training_data

    @staticmethod
    def _remove_untrainable_entities(example):
        example_entities = example.get("entities")
        if not example_entities:
            return None

        trainable_entities = []

        for entity in example_entities:
            if not is_trainable_extractor(entity.get("extractor")):
                logger.debug(
                    f"Excluding entity '{json.dumps(entity)}' from training data. "
                    f"Only entity examples extracted by a trainable "
                    f"extractor are dumped to training data."
                )
            else:
                trainable_entities.append(entity)

        example["entities"] = trainable_entities

    def _construct_valid_training_data_object(
        self,
        data: Dict[Text, Any],
        project_id: Text,
        filename: Text,
        username: Text,
        create_synonym_mappings: bool,
    ) -> TrainingData:
        _hash = common_utils.get_text_hash(data.get("text"))
        entities = []

        # remove untrainable entities
        DataService._remove_untrainable_entities(data)

        for e in data.get("entities", []):
            start = e.get("start")
            end = e.get("end")
            value = e.get("value")
            original_value = training_data_dict_get_entity(data, start, end)
            new_mapping_synonym_id = None

            # Check if one of the stored synonyms was used
            matching_synonym_value = (
                self.query(EntitySynonymValue)
                .filter(EntitySynonymValue.name == original_value)
                .first()
            )

            if create_synonym_mappings:
                # Add synonym entity mapping here

                entity_synonym_id = e.get("entity_synonym_id")

                if entity_synonym_id and not matching_synonym_value:
                    new_mapping = self.add_entity_synonym_mapped_values(
                        project_id, entity_synonym_id, [value]
                    )
                    new_mapping_synonym_id = entity_synonym_id if new_mapping else None

            entity = TrainingDataEntity(
                start=start,
                end=end,
                entity=e.get("entity"),
                value=value,
                original_value=original_value,
                extractor=e.get("extractor"),
                group=e.get("group"),
                role=e.get("role"),
                entity_synonym_id=matching_synonym_value.entity_synonym_id
                if matching_synonym_value
                else new_mapping_synonym_id,
            )

            entities.append(entity)

        return TrainingData(
            hash=_hash,
            text=data.get("text"),
            intent=data.get("intent"),
            annotator_id=username,
            annotated_at=time.time(),
            project_id=project_id,
            filename=filename,
            entities=entities,
        )

    @staticmethod
    def from_request(request: Request, **kwargs) -> "DataService":
        """Creates a `DataService` from an incoming HTTP request's DB session.

        Args:
            request: Incoming HTTP request.
            **kwargs: other key-value args, not used.

        Returns:
            `DataService` instance with a connection to the DB.
        """
        return DataService(request.ctx.db_session)


def training_data_dict_get_entity(
    training_data: Dict[Text, Any], start: Optional[int], end: Optional[int]
) -> Optional[Text]:
    """Extract the value of an entity inside a training data example.

    Args:
        training_data: Training data example as a dictionary.
        start: Start position of the entity value in the example's text.
        end: end position of the entity value in the example's text.

    Returns:
        Text value of the entity if `start` and `end` were provided, and `None`
            otherwise.
    """

    if start is not None and end is not None:
        return training_data["text"][start:end]

    return None


def format_entities(example: Dict[Text, Any]) -> List[Dict[Text, Any]]:
    """Extracts all entities from training example, and formats them.

    Args:
        example: Training_example with entities.

    Returns:
        Formatted entities.
    """
    ents = []
    for ent in example.get("entities", []):
        value = ent.get("value")
        if isinstance(value, dict):
            value = json.dumps(value)
        elif value is not None:
            value = str(value)
        built_entity = {
            "start": int(ent["start"]),
            "end": int(ent["end"]),
            "entity": ent["entity"],
            "value": value,
        }
        if ent.get("extractor"):
            built_entity["extractor"] = ent["extractor"]
        if ent.get("group"):
            built_entity["group"] = ent["group"]
        if ent.get("role"):
            built_entity["role"] = ent["role"]
        ents.append(built_entity)

    return ents


def nlu_format(
    data: List[Dict[Text, Any]],
    regex_features: Optional[List[Dict[Text, Any]]] = None,
    lookup_tables: Optional[List[Dict[Text, Any]]] = None,
    entity_synonyms: Optional[List[Dict[Text, Any]]] = None,
) -> Dict[Text, Dict[Text, Any]]:
    formatted_data = []
    for e in data:
        formatted_entities = format_entities(e)
        formatted_data.append(
            {"text": e["text"], "intent": e["intent"], "entities": formatted_entities}
        )

    rasa_nlu_data = {"common_examples": formatted_data}
    if regex_features:
        rasa_nlu_data["regex_features"] = regex_features
    if lookup_tables:
        rasa_nlu_data["lookup_tables"] = lookup_tables
    if entity_synonyms:
        rasa_nlu_data["entity_synonyms"] = entity_synonyms

    return {"rasa_nlu_data": rasa_nlu_data}
